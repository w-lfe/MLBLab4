{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DS-Aditya-928/CartPoleProject4/blob/main/RL_Project4_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6YBUjQ96xxib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b91a63-e8f6-42e7-8cfd-7a9c706bb0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/moviepy/config_defaults.py:1: DeprecationWarning: invalid escape sequence '\\P'\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: DeprecationWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/fx/painting.py:7: DeprecationWarning: Please import `sobel` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.filters import sobel\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "\n",
        "Imports! The first 3 are for our cartpole simulation, numpy is for our bot, tqdm is a super easy way to\n",
        "draw progress bars, and the last one is used to play the video of the simulation.\n",
        "\n",
        "'''\n",
        "import gymnasium as gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import moviepy.editor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üìå Before we go any further:\n",
        "\n",
        "Let's have a look over what we're trying to accomplish.\n",
        "\n",
        "<br>\n",
        "\n",
        "Reinforcement learning is a machine learning algorithm where we let the algorithm make its own descisions in an attempt to maximize a \"reward\". This reward can be a high score in a game, or more relevant to our project here, the time our algorithm is able to balance a pole mounted on a cart for.\n",
        "\n",
        "\n",
        "To train our algorithm, we're going to be using something called Q learning.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#üìå What is Q-learning?\n",
        "\n",
        "This bit is pretty complex, and this video explains Q learning better than I ever could :p\n",
        "\n",
        "https://www.youtube.com/watch?v=TiAXhVAZQl8  \n",
        "  \n",
        "<br>\n",
        "\n",
        "The basic idea however, is that we are going to maintain a table (appropriately called a Q table) that holds expected changes in score for all of our possible actions at a give state.\n",
        "\n",
        "<br>\n",
        "\n",
        "I.E, for the cart pole example, let's say the pole is 5 degrees off centre, and has a velocity of 1 m/s. Our algorithm is going to go to the corresponding cell in our table, and see which one of the possible actions (in the cartpole example, those are moving left or right) will result in the greatest increase in score. So, the cell holds an array with expected score changes for each action.\n",
        "\n",
        "Let's say the cell for the case described above has this array:  [-1.0, +2.5], with the first value corresponding to the expected change in score if we move to the left, and the other if we move to the right. Moving to the right gives us a score change of +2.5, so we're going to move to the right.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "The training bit here involves having the model change these values in the q table so that it makes better and better decisions over time. We'll delve more into the specifics of how to accomplish this later.\n",
        "\n",
        "<br>\n",
        "Got all that? It's ok if you didn't. I'm still wrapping my head around it myself. Feel free to ask us questions during office hours and USE THE INTERNET. It can teach you anything if you use it right.  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qz_KfKuDZdPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This here is the CartPoleBot class; all the functions you'll need to implement are in here.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UTVEvKm8U9-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import gym\n",
        "\n",
        "class CartPoleBot:\n",
        "    env: gym.Env\n",
        "    learningRate: float\n",
        "    discountFactor: float\n",
        "\n",
        "    def __init__(self,\n",
        "                 env: gym.Env,\n",
        "                 learningRate: float,\n",
        "                 initalEpsilon: float,\n",
        "                 epsilonDecay: float,\n",
        "                 finalEpsilon: float,\n",
        "                 discountFactor: float):\n",
        "        \"\"\"\n",
        "        Constructor. Don't change anything here. READ ALL THE COMMENTS THOUGH, they're hella useful.\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.learningRate = learningRate\n",
        "\n",
        "        # epsilon‚Äêgreedy params\n",
        "        self.epsilon = initalEpsilon\n",
        "        self.epsilonDecay = epsilonDecay\n",
        "        self.finalEpsilon = finalEpsilon\n",
        "\n",
        "        # initialize Q‚Äêtable as a dict mapping discretized states ‚Üí zeroed action‚Äêvalue arrays\n",
        "        self.qTable = defaultdict(lambda: np.zeros(self.env.action_space.n))\n",
        "\n",
        "        self.discountFactor = discountFactor\n",
        "\n",
        "\n",
        "    def discConv(self, obs):\n",
        "        \"\"\"\n",
        "        Discretize the continuous state vector into a hashable tuple of bins.\n",
        "        DO NOT CHANGE.\n",
        "        \"\"\"\n",
        "        posSpace  = np.linspace(-2.4, 2.4, 10)\n",
        "        velSpace  = np.linspace(-4, 4,     10)\n",
        "        angSpace  = np.linspace(-.2095, .2095, 10)\n",
        "        angVSpace = np.linspace(-4, 4,        10)\n",
        "        bins = [posSpace, velSpace, angSpace, angVSpace]\n",
        "\n",
        "        tR = []\n",
        "        for i in range(len(obs)):\n",
        "            tR.append(np.digitize(obs[i], bins[i]))\n",
        "        return tuple(tR)\n",
        "\n",
        "\n",
        "    def getAction(self, observation):\n",
        "        \"\"\"\n",
        "        Epsilon-greedy: with probability epsilon pick a random action,\n",
        "        otherwise pick the action with highest Q-value for the current state.\n",
        "        \"\"\"\n",
        "        state = self.discConv(observation)\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # explore\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            # exploit\n",
        "            return int(np.argmax(self.qTable[state]))\n",
        "\n",
        "\n",
        "    def update(self, pastObv, action, reward, terminated, currObv):\n",
        "        \"\"\"\n",
        "        Q-learning update rule:\n",
        "           Q(s,a) ‚Üê Q(s,a) + Œ± * [ R + Œ≥‚ãÖmax_a‚Ä≤ Q(s‚Ä≤,a‚Ä≤) ‚Äì Q(s,a) ]\n",
        "        If terminated=True, we treat future reward as zero.\n",
        "        \"\"\"\n",
        "        past_state = self.discConv(pastObv)\n",
        "        curr_state = self.discConv(currObv)\n",
        "\n",
        "        old_q      = self.qTable[past_state][action]\n",
        "        future_max = 0 if terminated else np.max(self.qTable[curr_state])\n",
        "\n",
        "        td_error = (reward + self.discountFactor * future_max) - old_q\n",
        "        self.qTable[past_state][action] = old_q + self.learningRate * td_error\n",
        "\n",
        "\n",
        "    def decayEpsilon(self):\n",
        "        \"\"\"\n",
        "        Reduce epsilon by epsilonDecay, but never go below finalEpsilon.\n",
        "        \"\"\"\n",
        "        self.epsilon = max(self.finalEpsilon, self.epsilon - self.epsilonDecay)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E6Cd3AeUerbl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Monkey‚Äêpatch numpy so that bool8 exists\n",
        "import numpy as np\n",
        "if not hasattr(np, \"bool8\"):\n",
        "    np.bool8 = np.bool_\n",
        "\n",
        "# 1) Imports\n",
        "import gym\n",
        "from gym.wrappers import RecordVideo\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class CartPoleBot:\n",
        "    def __init__(self,\n",
        "                 env: gym.Env,\n",
        "                 learningRate: float,\n",
        "                 initalEpsilon: float,\n",
        "                 epsilonDecay: float,\n",
        "                 finalEpsilon: float,\n",
        "                 discountFactor: float):\n",
        "        self.env = env\n",
        "        self.learningRate = learningRate\n",
        "        self.epsilon = initalEpsilon\n",
        "        self.epsilonDecay = epsilonDecay\n",
        "        self.finalEpsilon = finalEpsilon\n",
        "        self.qTable = defaultdict(lambda: np.zeros(self.env.action_space.n))\n",
        "        self.discountFactor = discountFactor\n",
        "\n",
        "    def discConv(self, obs):\n",
        "        posSpace  = np.linspace(-2.4, 2.4, 10)\n",
        "        velSpace  = np.linspace(-4, 4,     10)\n",
        "        angSpace  = np.linspace(-.2095, .2095, 10)\n",
        "        angVSpace = np.linspace(-4, 4,        10)\n",
        "        bins = [posSpace, velSpace, angSpace, angVSpace]\n",
        "        return tuple(np.digitize(obs[i], bins[i]) for i in range(len(obs)))\n",
        "\n",
        "    def getAction(self, observation):\n",
        "        state = self.discConv(observation)\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            return int(np.argmax(self.qTable[state]))\n",
        "\n",
        "    def update(self, pastObv, action, reward, terminated, currObv):\n",
        "        past_state = self.discConv(pastObv)\n",
        "        curr_state = self.discConv(currObv)\n",
        "        old_q      = self.qTable[past_state][action]\n",
        "        future_max = 0 if terminated else np.max(self.qTable[curr_state])\n",
        "        td_error   = (reward + self.discountFactor * future_max) - old_q\n",
        "        self.qTable[past_state][action] = old_q + self.learningRate * td_error\n",
        "\n",
        "    def decayEpsilon(self):\n",
        "        self.epsilon = max(self.finalEpsilon, self.epsilon - self.epsilonDecay)\n",
        "\n",
        "\n",
        "# 3) Wrap CartPole in RecordVideo\n",
        "env = RecordVideo(\n",
        "    gym.make(\"CartPole-v1\", render_mode=\"rgb_array\"),\n",
        "    \"/content\",\n",
        "    episode_trigger=lambda ep: ep % 5000 == 0,\n",
        "    new_step_api=True\n",
        ")\n",
        "\n",
        "# 4) Hyperparameters & Agent\n",
        "learningRate   = 0.05\n",
        "nEps           = 60_000\n",
        "startEpsilon   = 1.0\n",
        "epsilonDecay   = 1.0 / 30_000\n",
        "finalEpsilon   = 0.1\n",
        "discountFactor = 0.95\n",
        "\n",
        "balanceAgent = CartPoleBot(\n",
        "    env,\n",
        "    learningRate,\n",
        "    startEpsilon,\n",
        "    epsilonDecay,\n",
        "    finalEpsilon,\n",
        "    discountFactor\n",
        ")\n",
        "\n",
        "# 5) Training Loop with safe unpacking\n",
        "for episode in tqdm(range(nEps), desc=\"Episodes\"):\n",
        "    # reset can return 1, 2 or more values\n",
        "    reset_ret = env.reset()\n",
        "    if isinstance(reset_ret, tuple):\n",
        "        observation, info = reset_ret[:2]\n",
        "    else:\n",
        "        observation, info = reset_ret, {}\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = balanceAgent.getAction(observation)\n",
        "\n",
        "\n",
        "        step_ret = env.step(action)\n",
        "        if isinstance(step_ret, tuple):\n",
        "            if len(step_ret) == 5:\n",
        "                newObv, reward, terminated, truncated, info = step_ret\n",
        "            elif len(step_ret) == 4:\n",
        "                newObv, reward, done_flag, info = step_ret\n",
        "                terminated, truncated = done_flag, False\n",
        "            else:\n",
        "                raise RuntimeError(f\"Unexpected step() return: {step_ret}\")\n",
        "        else:\n",
        "            raise RuntimeError(\"env.step() did not return a tuple!\")\n",
        "\n",
        "        balanceAgent.update(observation, action, reward, terminated, newObv)\n",
        "        done = terminated or truncated\n",
        "        observation = newObv\n",
        "\n",
        "    balanceAgent.decayEpsilon()\n",
        "\n",
        "env.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "d-okFDFJyJ-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8789bb96-643d-491f-fb4c-f7291dc27f74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/wrappers/record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "\n",
            "Episodes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60000/60000 [8:25:51<00:00,  1.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#call this to play one of the generated mp4s. Replace N with the episode count. Or just download it idk im not ur dad\n",
        "moviepy.editor.ipython_display(\"/content/rl-video-episode-5000.mp4\")"
      ],
      "metadata": {
        "id": "daKctNGlvixI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "3b526fdc-456f-4f39-9e83-3faab5e7ee54"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<moviepy.video.io.html_tools.HTML2 object>"
            ],
            "text/html": [
              "<div align=middle><video src='data:video/mp4;base64,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' controls>Sorry, seems like your browser doesn't support HTML5 audio/video</video></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}